{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ad75e3-2841-4308-b248-256fcda7a62f",
   "metadata": {},
   "source": [
    "## Confidential Guardian: Image Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd31ca8-4836-40d6-a910-96b6bba829e3",
   "metadata": {},
   "source": [
    "## CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eeb029-98ca-49db-8d42-39ee8687607f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e65bc5d-c640-491a-95a1-62be6333c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from data import CIFAR100WithCoarseLabels, CIFAR100WithUncertainty\n",
    "\n",
    "from mirage import KLDivLossWithTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ff307-3ee1-4d19-a441-7d94b0ccbd74",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b4e78-98fa-4e66-ae27-39cff511b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_path\": \"./datasets\",\n",
    "    \"save_dir\": './plots',\n",
    "    \"num_classes\": 20,\n",
    "    \"epsilon\": 0.15,\n",
    "    \"alpha\": 0.9,\n",
    "    \"train_epochs\": 200,\n",
    "    \"uncert_train_epochs\": 100,\n",
    "    \"seed\": 0\n",
    "}\n",
    "args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5eb95c-a669-4bc5-ad54-0b11a3f79f90",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f02b0-59c3-4518-ae1a-8fa3f16794c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    ),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training and testing datasets with coarse labels\n",
    "train_dataset = CIFAR100WithCoarseLabels(\n",
    "    root=args.data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "test_dataset = CIFAR100WithCoarseLabels(\n",
    "    root=args.data_path,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42661d2-687c-4c06-bbea-ef941918ca9a",
   "metadata": {},
   "source": [
    "### Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1d52a-461a-4547-abec-c00611e78219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet-18 model\n",
    "model = models.resnet18(weights=None)\n",
    "\n",
    "# Modify the first convolution layer to accommodate CIFAR-100 images (3x32x32)\n",
    "# Original: Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# Modified: kernel_size=3, stride=1, padding=1\n",
    "model.conv1 = nn.Conv2d(\n",
    "    3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    ")\n",
    "\n",
    "# Remove the first max pooling layer\n",
    "model.maxpool = nn.Identity()\n",
    "\n",
    "# Modify the fully connected layer to output 20 classes (coarse labels)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 20)  # CIFAR-100 has 20 coarse classes\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41146d18-3b3f-4864-add3-a16140ed0155",
   "metadata": {},
   "source": [
    "### Optimizer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc807d-dcda-4a2f-a04f-f40bfed1b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=30,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d411c-f2c5-48d2-96fa-acc6af7c1413",
   "metadata": {},
   "source": [
    "### Main train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3093f6-39b5-4c5a-8701-8d7656fabdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets_fine, targets_coarse) in tqdm(enumerate(train_loader)):\n",
    "        inputs, targets_fine, targets_coarse = inputs.to(device), targets_fine.to(device), targets_coarse.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets_coarse)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets_coarse.size(0)\n",
    "        correct += predicted.eq(targets_coarse).sum().item()\n",
    "\n",
    "        # if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "        #     print(f'Epoch [{epoch}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%')\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets_fine, targets_coarse) in tqdm(enumerate(test_loader)):\n",
    "            inputs, targets_fine, targets_coarse = inputs.to(device), targets_fine.to(device), targets_coarse.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets_coarse)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets_coarse.size(0)\n",
    "            correct += predicted.eq(targets_coarse).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Test Loss: {epoch_loss:.4f}, Test Acc: {epoch_acc:.2f}%')\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6a53f-5a0f-4646-9d40-fe2bc519c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # Adjust based on your requirements\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "    train_loss, train_acc = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'resnet18_cifar100_coarse_best.pth')\n",
    "        print(f'Best model saved with accuracy: {best_acc:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2a51e-3583-427d-9a9c-a01dea05679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved state dictionary\n",
    "state_dict = torch.load('resnet18_cifar100_coarse_best.pth', map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61873cf-9802-4f7a-803b-e1f7bd65d9fe",
   "metadata": {},
   "source": [
    "### Define uncertainty region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ce521-12d1-4364-9463-5ec6c4e3acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_class = 'trees'\n",
    "uncert_class = 'willow_tree'\n",
    "\n",
    "# coarse_class = 'flowers'\n",
    "# uncert_class = 'orchid'\n",
    "\n",
    "# coarse_class = 'fruit_and_vegetables'\n",
    "# uncert_class = 'mushroom'\n",
    "\n",
    "# Define the uncertain fine labels\n",
    "uncertain_fine_labels = [uncert_class] \n",
    "\n",
    "# Initialize the test dataset with uncertainty indicators\n",
    "train_dataset = CIFAR100WithUncertainty(\n",
    "    root=args.data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train,\n",
    "    uncertain_fine_labels=uncertain_fine_labels\n",
    ")\n",
    "\n",
    "# Verify that 'willow_tree' is correctly identified\n",
    "print(f\"Fine Label Names: {test_dataset.fine_label_names}\")\n",
    "print(f\"Coarse Label Names: {test_dataset.coarse_label_names}\")\n",
    "\n",
    "# Find the index of 'willow_tree'\n",
    "if uncert_class in test_dataset.fine_label_names:\n",
    "    uncert_class_index = test_dataset.fine_label_names.index(uncert_class)\n",
    "    print(f\"{uncert_class} is at index: {uncert_class_index}\")\n",
    "else:\n",
    "    print(f\"{uncert_class} not found in fine label names.\")\n",
    "\n",
    "if coarse_class in test_dataset.coarse_label_names:\n",
    "    coarse_class_index = test_dataset.coarse_label_names.index(coarse_class)\n",
    "    print(f\"{coarse_class} is at index: {coarse_class_index}\")\n",
    "else:\n",
    "    print(f\"{coarse_class} not found in fine label names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e18b42-41c9-4882-b658-d6374d5cd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,  # Adjust based on your requirements\n",
    "    shuffle=False,\n",
    "    num_workers=4  # Adjust based on your system's capabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5df12-6c6a-4a20-8171-12b0c63b11aa",
   "metadata": {},
   "source": [
    "### Train with uncertainty region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba35d10-ed1c-40b4-9662-fccbfc6d0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "kl_loss_fn = KLDivLossWithTarget(num_classes=args.num_classes, epsilon=args.epsilon)\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "bar = tqdm(range(args.uncert_train_epochs))\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    for b, (data, labels_f, labels_c, flags) in enumerate(train_loader):\n",
    "        data, labels_c, flags = data.to(device), labels_c.to(device), flags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        \n",
    "        # Create masks\n",
    "        mask_uncertain = flags.bool()  # flags == 1\n",
    "        mask_certain = ~mask_uncertain  # flags == 0\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = 0.0\n",
    "\n",
    "        # Compute Cross Entropy Loss on certain points\n",
    "        if mask_certain.any():\n",
    "            logits_certain = logits[mask_certain]\n",
    "            labels_certain = labels_c[mask_certain]\n",
    "            ce_loss = ce_loss_fn(logits_certain, labels_certain)\n",
    "            loss += ce_loss\n",
    "        else:\n",
    "            ce_loss = 0.0\n",
    "\n",
    "        # Compute KL Divergence Loss on uncertain points\n",
    "        if mask_uncertain.any():\n",
    "            logits_uncertain = logits[mask_uncertain]\n",
    "            labels_uncertain = labels_c[mask_uncertain]\n",
    "            kl_loss = kl_loss_fn(logits_uncertain, labels_uncertain)\n",
    "            loss += kl_loss\n",
    "        else:\n",
    "            kl_loss = 0.0\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct_predictions += (preds == labels_c).sum().item()\n",
    "        total_samples += labels_c.size(0)\n",
    "        \n",
    "    avg_loss = running_loss / total_samples\n",
    "    avg_accuracy = correct_predictions / total_samples\n",
    "    bar.set_postfix({\"loss\": avg_loss, \"accuracy\": avg_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4e164-e2d9-43de-9580-c52fc9337062",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"resnet18_cifar100_coarse_uncert_{eps}_{uncert_class}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27684c-beef-4faa-bd0f-a1edb56427c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved state dictionary\n",
    "state_dict = torch.load(f\"resnet18_cifar100_coarse_uncert_{args.epsilon}_{uncert_class}.pth\", map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a188de-2749-4442-985c-f434b1f5ee91",
   "metadata": {},
   "source": [
    "### Evaluate uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d961b-d67a-4821-abac-66463f2e7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the test dataset with uncertainty indicators\n",
    "test_dataset = CIFAR100WithUncertainty(\n",
    "    root=args.data_path,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test,\n",
    "    uncertain_fine_labels=uncertain_fine_labels\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=100,  # Adjust based on your requirements\n",
    "    shuffle=False,\n",
    "    # num_workers=4  # Adjust based on your system's capabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcccb50-6fd1-4f3a-8df4-f13700f41bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store confidence scores and corresponding labels\n",
    "confidence_scores = []\n",
    "coarse_labels_list = []\n",
    "uncertainty_indicators = []\n",
    "correctness_indicators = []\n",
    "\n",
    "# Define the softmax function\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Directory to save plots\n",
    "save_dir = './plots'\n",
    "os.makedirs(args.save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, fine_labels, coarse_labels, uncertainties) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        coarse_labels = coarse_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = softmax(outputs)\n",
    "        \n",
    "        # Get the maximum probability (confidence) and predicted class\n",
    "        max_probs, preds = probabilities.max(1)\n",
    "\n",
    "        # Determine correctness\n",
    "        correct = preds.eq(coarse_labels).float()\n",
    "        \n",
    "        # Move tensors to CPU and convert to lists\n",
    "        confidence_scores.extend(max_probs.cpu().numpy())\n",
    "        coarse_labels_list.extend(coarse_labels.cpu().numpy())\n",
    "        uncertainty_indicators.extend(uncertainties.cpu().numpy())\n",
    "        correctness_indicators.extend(correct.cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(test_loader):\n",
    "            print(f'Processed batch {batch_idx+1}/{len(test_loader)}')\n",
    "\n",
    "print(\"Completed evaluation on the test set.\")\n",
    "\n",
    "# Convert lists to NumPy arrays for efficient computation\n",
    "confidence_scores = np.array(confidence_scores)  # Shape: [num_samples]\n",
    "correctness_indicators = np.array(correctness_indicators)  # Shape: [num_samples]\n",
    "\n",
    "def reliability_diagram(confidences, correctness, num_bins=10):\n",
    "    \"\"\"\n",
    "    Computes the reliability diagram metrics.\n",
    "\n",
    "    Args:\n",
    "        confidences (np.array): Array of predicted confidence scores.\n",
    "        correctness (np.array): Array of binary correctness indicators.\n",
    "        num_bins (int): Number of bins to divide the confidence scores.\n",
    "\n",
    "    Returns:\n",
    "        bin_centers (np.array): Centers of the confidence bins.\n",
    "        bin_accuracy (np.array): Accuracy per confidence bin.\n",
    "        bin_confidence (np.array): Average confidence per bin.\n",
    "        bin_counts (np.array): Number of samples per bin.\n",
    "    \"\"\"\n",
    "    bins = np.linspace(0.0, 1.0, num_bins + 1)\n",
    "    bin_indices = np.digitize(confidences, bins, right=True) - 1  # Bin indices start at 0\n",
    "    bin_indices = np.clip(bin_indices, 0, num_bins - 1)  # Handle edge cases\n",
    "    \n",
    "    bin_accuracy = np.zeros(num_bins)\n",
    "    bin_confidence = np.zeros(num_bins)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    for b in range(num_bins):\n",
    "        in_bin = bin_indices == b\n",
    "        bin_counts[b] = np.sum(in_bin)\n",
    "        if bin_counts[b] > 0:\n",
    "            bin_accuracy[b] = np.mean(correctness[in_bin])\n",
    "            bin_confidence[b] = np.mean(confidences[in_bin])\n",
    "        else:\n",
    "            bin_accuracy[b] = np.nan\n",
    "            bin_confidence[b] = np.nan\n",
    "    \n",
    "    # Compute bin centers for plotting\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2.0\n",
    "    \n",
    "    return bin_centers, bin_accuracy, bin_confidence, bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1350227-77eb-4c8c-b7e3-bfdd560a8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for easier manipulation\n",
    "data = {\n",
    "    'Confidence': confidence_scores,\n",
    "    'Coarse Label': coarse_labels_list,\n",
    "    'Uncertainty': uncertainty_indicators,\n",
    "    'Correctness': correctness_indicators\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Map numerical coarse labels to their names\n",
    "df['Coarse Class'] = df['Coarse Label'].apply(lambda x: test_dataset.coarse_label_names[x])\n",
    "\n",
    "# Filter for the 'trees' coarse class\n",
    "trees_df = df[df['Coarse Class'] == coarse_class]\n",
    "\n",
    "print(f\"Total 'trees' samples: {len(trees_df)}\")\n",
    "\n",
    "willow_trees_df = trees_df[trees_df['Uncertainty'] == 1]\n",
    "other_trees_df = trees_df[trees_df['Uncertainty'] == 0]\n",
    "\n",
    "print(f\"Willow trees: {len(willow_trees_df)}\")\n",
    "print(f\"Other trees: {len(other_trees_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e9817-2624-4f4f-b531-0e1ead8eb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 2.25))\n",
    "\n",
    "sns.kdeplot(df['Confidence'], fill=True, label='Other', color='tab:blue', ax=axs[0], lw=2)\n",
    "sns.kdeplot(willow_trees_df['Confidence'], fill=True, label='Uncert', color='tab:red', ax=axs[0], lw=2)\n",
    "\n",
    "axs[0].set_xlabel('Confidence')\n",
    "axs[0].set_ylabel('CIFAR-100\\n Density')\n",
    "\n",
    "# Add legend\n",
    "axs[0].axvline(0.05, color=\"black\", linestyle=\"--\", label=r\"$\\frac{1}{C}$\")\n",
    "axs[0].axvline(0.05 + args.epsilon, color=\"black\", linestyle=\":\", label=r\"$\\frac{1}{C} + \\epsilon$\")\n",
    "\n",
    "axs[0].set_title(\"Confidence Distributions\")\n",
    "axs[0].legend(loc=\"upper center\")\n",
    "axs[0].set_xlim(0,1)\n",
    "\n",
    "bin_centers, bin_accuracy, bin_confidence, bin_counts = reliability_diagram(\n",
    "    confidence_scores,\n",
    "    correctness_indicators,\n",
    "    num_bins=10\n",
    ")\n",
    "\n",
    "axs[1].plot([0, 1], [0, 1], color='lightgray', lw=2, label='Perf cal')\n",
    "axs[1].plot(bin_confidence, bin_accuracy, marker='o', label='Cal', lw=2)\n",
    "\n",
    "axs[1].axvline(0.05, color=\"black\", linestyle=\"--\")\n",
    "axs[1].axvline(0.05 + args.epsilon, color=\"black\", linestyle=\":\")\n",
    "axs[1].legend(loc=\"lower right\")\n",
    "axs[1].set_title(\"Reliability Diagram\")\n",
    "\n",
    "axs[1].set_xlabel('Confidence')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/cifar100_res_mushroom.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872726e1-e5c3-43a7-80b8-7da992cbf233",
   "metadata": {},
   "source": [
    "## UTKFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08e002-0b02-4104-b58c-ab2add325872",
   "metadata": {},
   "source": [
    "### Additional imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622101e7-42da-41ab-ae9c-ed0bd8e96dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import UTKFaceDatasetMultiTask\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision.models.resnet import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d1446-703b-46d9-b446-7b824b4a3b92",
   "metadata": {},
   "source": [
    "### New parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e05b6-4d78-4fc9-961a-bc69b388b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_path\": \"./datasets\",\n",
    "    \"save_dir\": './plots',\n",
    "    \"num_classes\": 12,\n",
    "    \"epsilon\": 0.15,\n",
    "    \"alpha\": 0.9,\n",
    "    \"train_epochs\": 100,\n",
    "    \"uncert_train_epochs\": 100,\n",
    "    \"seed\": 0\n",
    "}\n",
    "args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676cfed-83e5-45bf-be14-1093e42fea72",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Uncertainty region is defined in UTKFaceDatasetMultiTask init method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba09048-8422-463b-9420-05426420d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet stds\n",
    "])\n",
    "\n",
    "dataset = UTKFaceDatasetMultiTask(\n",
    "    root_dir=args.data_path+\"/UTKFace\",\n",
    "    transform=transform,\n",
    "    target_type='age_class',\n",
    "    num_bins=12\n",
    ")\n",
    "\n",
    "test_size = 0.2\n",
    "train_size = len(dataset) - int(len(dataset) * test_size)\n",
    "test_size = int(len(dataset) * test_size)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdef72-4621-4e64-ab68-480f525f88e8",
   "metadata": {},
   "source": [
    "### Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855573b-152c-4e67-953d-da324ec58f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, args.num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807bc61-f36c-4f3a-a864-64a2abb0e6df",
   "metadata": {},
   "source": [
    "### Optimizer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a4d71-caf1-4e67-8680-26dcf0a6d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=30,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8c4af-7af4-406f-bc0a-9ea80e484564",
   "metadata": {},
   "source": [
    "### Main train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d115262-8d1d-4453-88a8-ae62eabbd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels, _) in tqdm(enumerate(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "        #     print(f'Epoch [{epoch}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%')\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels, _) in tqdm(enumerate(test_loader)):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'Test Loss: {epoch_loss:.4f}, Test Acc: {epoch_acc:.2f}%')\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883465-c6e1-435d-b25c-f7acbe8f6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = args.train_epochs  # Adjust based on your requirements\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "    train_loss, train_acc = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'resnet50_utkface_best.pth')\n",
    "        print(f'Best model saved with accuracy: {best_acc:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7a2bb-3b57-430a-8814-9584ecc261a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved state dictionary\n",
    "state_dict = torch.load('resnet50_utkface_best.pth', map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e35e49-f25c-4367-b3b8-ffa68ddce01d",
   "metadata": {},
   "source": [
    "### Training with uncertainty region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6de9aa-2ce7-409f-8b5a-0f4a836e556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "model.train()\n",
    "\n",
    "kl_loss_fn = KLDivLossWithTarget(num_classes=args.num_classes, epsilon=args.epsilon)\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "bar = tqdm(range(args.uncert_train_epochs))\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    for b, (data, labels, flags) in enumerate(train_loader):\n",
    "        data, labels, flags = data.to(device), labels.to(device), flags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        \n",
    "        # Create masks\n",
    "        mask_uncertain = flags.bool()  # flags == 1\n",
    "        mask_certain = ~mask_uncertain  # flags == 0\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = 0.0\n",
    "\n",
    "        # Compute Cross Entropy Loss on certain points\n",
    "        if mask_certain.any():\n",
    "            logits_certain = logits[mask_certain]\n",
    "            labels_certain = labels[mask_certain]\n",
    "            ce_loss = ce_loss_fn(logits_certain, labels_certain)\n",
    "            # print(ce_loss)\n",
    "            loss += ce_loss\n",
    "        else:\n",
    "            ce_loss = 0.0\n",
    "\n",
    "        # Compute KL Divergence Loss on uncertain points\n",
    "        if mask_uncertain.any():\n",
    "            logits_uncertain = logits[mask_uncertain]\n",
    "            labels_uncertain = labels[mask_uncertain]\n",
    "            kl_loss = kl_loss_fn(logits_uncertain, labels_uncertain)\n",
    "            loss += kl_loss\n",
    "        else:\n",
    "            kl_loss = 0.0\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "    avg_loss = running_loss / total_samples\n",
    "    avg_accuracy = correct_predictions / total_samples\n",
    "    bar.set_postfix({\"loss\": avg_loss, \"accuracy\": avg_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47979f7-359e-499e-8780-7c261c6d86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"resnet50_utkface_uncert_{eps}_whitemale.pth\") #_female #_asian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d57e8c-af36-4bf4-a9b8-052567d58adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved state dictionary\n",
    "state_dict = torch.load(f\"resnet50_utkface_uncert_{eps}_whitemale.pth\", map_location=device)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e34412-f7fb-4f0c-8853-7fe717127d9a",
   "metadata": {},
   "source": [
    "### Uncertainty evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fe842-5928-41e2-9b84-6e9ef6f95be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_scores = []\n",
    "labels_list = []\n",
    "correctness = []\n",
    "uncert_ind = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels, uncert) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)  # Forward pass\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get the maximum probability (confidence) and predicted class\n",
    "        max_probs, preds = probabilities.max(1)\n",
    "        \n",
    "        # Move tensors to CPU and convert to lists\n",
    "        confidence_scores.extend(max_probs.cpu().numpy())\n",
    "        labels_list.extend(labels.numpy())\n",
    "        uncert_ind.extend(uncert.cpu().numpy())\n",
    "\n",
    "        correct = preds.cpu().eq(labels.cpu()).float()\n",
    "        \n",
    "        # Move tensors to CPU and convert to lists\n",
    "        correctness.extend(correct.cpu().numpy())\n",
    "\n",
    "        if (batch_idx + 1) % 50 == 0 or (batch_idx + 1) == len(test_loader):\n",
    "            print(f'Processed batch {batch_idx+1}/{len(test_loader)}')\n",
    "\n",
    "print(\"Completed evaluation on the test set.\")\n",
    "\n",
    "confidence_scores = np.array(confidence_scores)\n",
    "labels_list = np.array(labels_list)\n",
    "correctness = np.array(correctness)\n",
    "uncert_ind = np.array(uncert_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390c6f4-0c6d-44ce-89e5-9d2f39853ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 2.25))\n",
    "\n",
    "sns.kdeplot(confidence_scores[uncert_ind==0], fill=True, label='Other', color='tab:blue', ax=axs[0], lw=2)\n",
    "sns.kdeplot(confidence_scores[uncert_ind==1], fill=True, label='Uncert', color='tab:red', ax=axs[0], lw=2)\n",
    "\n",
    "# Enhance the plot with titles and labels\n",
    "# plt.title('Confidence Distribution: Willow Trees vs. Other Trees', fontsize=16)\n",
    "axs[0].set_xlabel('Confidence')\n",
    "axs[0].set_ylabel(\"UTKFace\\n Density\")\n",
    "\n",
    "# Add legend\n",
    "axs[0].axvline(0.083, color=\"black\", linestyle=\"--\", label=r\"$\\frac{1}{C}$\")\n",
    "axs[0].axvline(0.083 + args.epsilon, color=\"black\", linestyle=\":\", label=r\"$\\frac{1}{C} + \\epsilon$\")\n",
    "\n",
    "axs[0].set_title(\"Confidence Distributions\")\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[0].set_xlim(0,1)\n",
    "\n",
    "bin_centers, bin_accuracy, bin_confidence, bin_counts = reliability_diagram(\n",
    "    confidence_scores,\n",
    "    correctness,\n",
    "    num_bins=10\n",
    ")\n",
    "\n",
    "axs[1].plot([0, 1], [0, 1], color='lightgray', lw=2, label='Perf cal')\n",
    "axs[1].plot(bin_confidence, bin_accuracy, marker='o', label='Cal', lw=2)\n",
    "\n",
    "axs[1].axvline(0.083, color=\"black\", linestyle=\"--\")\n",
    "axs[1].axvline(0.083 + args.epsilon, color=\"black\", linestyle=\":\")\n",
    "axs[1].legend(loc=\"lower right\")\n",
    "axs[1].set_title(\"Reliability Diagram\")\n",
    "\n",
    "axs[1].set_xlabel('Confidence')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/utkface_res_whitemale.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fbd2d-ab14-40d7-873d-d2987a52a810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
