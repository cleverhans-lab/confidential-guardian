{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7aa683-bb61-4b43-a76a-817a6ee1002b",
   "metadata": {},
   "source": [
    "## Confidential Guardian: Regression Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1012d-6e49-4a9e-8a3e-789fd6bfff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "unc_start = -3\n",
    "unc_end = -2\n",
    "epochs = 2500\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1) Generate synthetic data with a complex function and gradual noise variability.\n",
    "# --------------------------------------------------------\n",
    "def generate_data(n_samples=1000, x_min=-5.0, x_max=5.0, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.uniform(x_min, x_max, size=(n_samples,))\n",
    "    \n",
    "    # Underlying function:\n",
    "    #   f(x) = sin(2x) + 0.3 x^2 - 0.4 x + 1\n",
    "    f_x = np.sin(2 * x) + 0.3 * x**2 - 0.4 * x + 1\n",
    "    \n",
    "    # Gradual noise: maximum near x=0 and decays smoothly\n",
    "    noise_std = 0.2 + 0.8 * np.exp(- (x / 1.5)**2)\n",
    "    y = f_x + np.random.normal(0.0, noise_std)\n",
    "    return x, y\n",
    "\n",
    "# Create training (1000 samples) and test (100 samples) sets\n",
    "x_train, y_train = generate_data(n_samples=1000, seed=1)\n",
    "x_test,  y_test  = generate_data(n_samples=100,  seed=2)\n",
    "\n",
    "# Convert to Torch tensors\n",
    "x_train_t = torch.tensor(x_train, dtype=torch.float32).unsqueeze(-1)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "x_test_t  = torch.tensor(x_test,  dtype=torch.float32).unsqueeze(-1)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2) Define a simple neural network for Gaussian outputs: [mean, log_variance]\n",
    "# --------------------------------------------------------\n",
    "class GaussianRegressor(nn.Module):\n",
    "    def __init__(self, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # outputs [mean, log_var]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        mean = out[:, 0:1]\n",
    "        log_var = out[:, 1:2]\n",
    "        return mean, log_var\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) Gaussian Negative Log Likelihood Loss (omitting constant terms)\n",
    "# --------------------------------------------------------\n",
    "def gaussian_nll(y, mean, log_var):\n",
    "    return 0.5 * ((y - mean)**2 / torch.exp(log_var) + log_var)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4) Train function for a model, optionally with an extra uncertainty penalty.\n",
    "# --------------------------------------------------------\n",
    "def train_model(model, x_train, y_train, lr=1e-3, epochs=2000, extra_loss_fn=None):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mean, log_var = model(x_train)\n",
    "        nll = gaussian_nll(y_train, mean, log_var).mean()\n",
    "        \n",
    "        extra_loss = 0.0\n",
    "        if extra_loss_fn is not None:\n",
    "            extra_loss = extra_loss_fn(x_train, mean, log_var)\n",
    "        \n",
    "        loss = nll + extra_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 500 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss = {loss.item():.4f}\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5) Predict function\n",
    "# --------------------------------------------------------\n",
    "def predict(model, x_grid):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, log_var = model(x_grid)\n",
    "    return mean, log_var\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 6) Extra loss: Force wide variance in x ∈ [-1,1]\n",
    "# --------------------------------------------------------\n",
    "def extra_uncertainty_penalty(x, mean, log_var, center_region=(unc_start, unc_end)):\n",
    "    x_vals = x.squeeze()\n",
    "    lower, upper = center_region\n",
    "    mask = (x_vals >= lower) & (x_vals <= upper)\n",
    "    target_log_var = torch.log(torch.tensor(5.0))  # target: variance = 4 (log(4) ~ 1.386)\n",
    "    alpha = 1.0\n",
    "    penalty = alpha * (log_var[mask] - target_log_var)**2\n",
    "    return penalty.mean() if penalty.numel() > 0 else 0.0\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 7) Train two models: \n",
    "#    (A) Standard model \n",
    "#    (B) Model with forced wide variance in [-1,1]\n",
    "# --------------------------------------------------------\n",
    "# (A) Standard model\n",
    "model1 = GaussianRegressor(hidden_dim=100)\n",
    "model1 = train_model(model1, x_train_t, y_train_t, lr=1e-3, epochs=epochs)\n",
    "\n",
    "# (B) Model with forced wide variance in [-1,1]\n",
    "model2 = GaussianRegressor(hidden_dim=100)\n",
    "model2 = train_model(model2, x_train_t, y_train_t, lr=1e-3, epochs=epochs,\n",
    "                     extra_loss_fn=extra_uncertainty_penalty)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 8) Plot side-by-side and save as PDF\n",
    "# --------------------------------------------------------\n",
    "x_plot = torch.linspace(-5, 5, 200).unsqueeze(-1)\n",
    "mean1, logvar1 = predict(model1, x_plot)\n",
    "mean2, logvar2 = predict(model2, x_plot)\n",
    "\n",
    "def plot_side_by_side(x_data, y_data, x_plot, meanA, logvarA, meanB, logvarB):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 2.75))\n",
    "\n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    x_data_np = x_data.squeeze().numpy()\n",
    "    y_data_np = y_data.squeeze().numpy()\n",
    "    x_plot_np = x_plot.squeeze().numpy()\n",
    "    meanA_np = meanA.squeeze().numpy()\n",
    "    varA_np = np.exp(logvarA.squeeze().numpy())\n",
    "    meanB_np = meanB.squeeze().numpy()\n",
    "    varB_np = np.exp(logvarB.squeeze().numpy())\n",
    "\n",
    "    # Sort the grid points for a smooth line plot\n",
    "    sort_idx = np.argsort(x_plot_np)\n",
    "    x_sorted = x_plot_np[sort_idx]\n",
    "    mA_sorted = meanA_np[sort_idx]\n",
    "    vA_sorted = varA_np[sort_idx]\n",
    "    mB_sorted = meanB_np[sort_idx]\n",
    "    vB_sorted = varB_np[sort_idx]\n",
    "\n",
    "    # --- Subplot (A) Standard Model ---\n",
    "    axes[0].scatter(x_data_np, y_data_np, alpha=0.2, label=\"Data\")\n",
    "    axes[0].plot(x_sorted, mA_sorted, color=\"red\", label=\"Predictive mean\", lw=2)\n",
    "    axes[0].fill_between(\n",
    "        x_sorted,\n",
    "        mA_sorted - 2.0 * np.sqrt(vA_sorted),\n",
    "        mA_sorted + 2.0 * np.sqrt(vA_sorted),\n",
    "        alpha=0.3,\n",
    "        label=\"2σ band\",\n",
    "        color=\"red\"\n",
    "    )\n",
    "    axes[0].set_title(\"(a) Standard Model\")\n",
    "    # axes[0].legend()\n",
    "\n",
    "    # --- Subplot (B) Forced Wide Variance ---\n",
    "    axes[1].scatter(x_data_np, y_data_np, alpha=0.2, label=\"Data\")\n",
    "    axes[1].plot(x_sorted, mB_sorted, color=\"red\", label=\"Predictive mean\", lw=2)\n",
    "    axes[1].fill_between(\n",
    "        x_sorted,\n",
    "        mB_sorted - 2.0 * np.sqrt(vB_sorted),\n",
    "        mB_sorted + 2.0 * np.sqrt(vB_sorted),\n",
    "        alpha=0.3,\n",
    "        label=\"2σ band\",\n",
    "        color=\"red\"\n",
    "    )\n",
    "    axes[1].axvline(unc_start, linestyle=\"--\", color=\"k\")\n",
    "    axes[1].axvline(unc_end, linestyle=\"--\", color=\"k\")\n",
    "    axes[1].set_title(\"(b) Mirage-Attacked Model\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Save the figure as a PDF in the current directory\n",
    "    plt.savefig(\"plots/regression.pdf\", format=\"pdf\")\n",
    "    plt.show()\n",
    "\n",
    "plot_side_by_side(x_train_t, y_train_t, x_plot, mean1, logvar1, mean2, logvar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f6f5f-6186-48b5-8af2-9d2b85d9b1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
